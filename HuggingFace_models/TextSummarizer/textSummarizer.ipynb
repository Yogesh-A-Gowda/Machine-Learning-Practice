{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "your_token = os.getenv(\"HF\")\n",
    "# Log in using the token\n",
    "login(token=your_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "#first run the following line, once its executed, comment it out, and then change the model path.\n",
    "# models would be stored in C:/yogesh.gowda/cache/huggingface/hub/ -> cut and paste the model folder to your desired location and provide the path below.\n",
    "\n",
    "#pipe = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "#Make sure path lenght is less than 96 characters for Windows OS, and prefix the path with r\n",
    "\n",
    "\n",
    "    \n",
    "pipe = pipeline(\"summarization\", model=r\"C:\\models--sshleifer--distilbart-cnn-12-6\\snapshots\\a4f8f3ea906ed274767e9906dbaede7531d660ff\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = '''Markov Decision Process (MDP) in Reinforcement Learning\n",
    "Last Updated : 24 Feb, 2025\n",
    "Markov Decision Process is a mathematical framework used to describe an environment in decision-making scenarios where outcomes are partly random and partly under the control of a decision-maker. MDPs provide a formalism for modeling decision-making in situations where outcomes are uncertain, making them essential for reinforcement learning.\n",
    "\n",
    "Components of an MDP\n",
    "An MDP is defined by a tuple \n",
    "(\n",
    "S\n",
    ",\n",
    "A\n",
    ",\n",
    "P\n",
    ",\n",
    "R\n",
    ",\n",
    "γ\n",
    ")\n",
    "(S,A,P,R,γ) where:\n",
    "\n",
    "S (State Space): A finite or infinite set of states representing the environment.\n",
    "A (Action Space): A set of actions available to the agent in each state.\n",
    "P (Transition Probability): A probability function P(s' | s, a) that defines the likelihood of transitioning from state s to s' after taking action a.\n",
    "R (Reward Function): A function R(s, a, s') that assigns a reward for moving from state s to s' via action a.\n",
    "γ (Discount Factor): A value in the range [0,1] that determines the importance of future rewards.\n",
    "Markov Property\n",
    "MDP follows the Markov Property, which states that the next state depends only on the current state and action, not on past states.\n",
    "\n",
    "Mathematically,\n",
    "\n",
    "P\n",
    "(\n",
    "s\n",
    "′\n",
    "∣\n",
    "s\n",
    ",\n",
    "a\n",
    ",\n",
    "s\n",
    "t\n",
    "−\n",
    "1\n",
    ",\n",
    "a\n",
    "t\n",
    "−\n",
    "1\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ")\n",
    "=\n",
    "P\n",
    "(\n",
    "s\n",
    "′\n",
    "∣\n",
    "s\n",
    ",\n",
    "a\n",
    ")\n",
    "P(s \n",
    "′\n",
    " ∣s,a,s \n",
    "t−1\n",
    "​\n",
    " ,a \n",
    "t−1\n",
    "​\n",
    " ,...)=P(s \n",
    "′\n",
    " ∣s,a)\n",
    "\n",
    "This property ensures that MDPs can be efficiently solved using mathematical techniques like dynamic programming.\n",
    "\n",
    "Policy\n",
    "A policy defines the agent’s strategy for selecting actions in each state. It can be:\n",
    "\n",
    "Deterministic (\n",
    "π\n",
    "(\n",
    "s\n",
    ")\n",
    "π(s)): Always selects a fixed action for a given state.\n",
    "Stochastic (\n",
    "π\n",
    "(\n",
    "a\n",
    "∣\n",
    "s\n",
    ")\n",
    "π(a∣s)): Assigns probabilities to different actions for a given state.\n",
    "Value Functions and Optimality\n",
    "MDPs aim to find an optimal policy that maximizes cumulative rewards. Two key functions help evaluate policies:\n",
    "\n",
    "1. State Value Function (\n",
    "V\n",
    "π\n",
    "(\n",
    "s\n",
    ")\n",
    "Vπ(s)):\n",
    "\n",
    "V\n",
    "π\n",
    "(\n",
    "s\n",
    ")\n",
    "=\n",
    "E\n",
    "[\n",
    "∑\n",
    "t\n",
    "=\n",
    "0\n",
    "∞\n",
    "γ\n",
    "t\n",
    "R\n",
    "(\n",
    "s\n",
    "t\n",
    ",\n",
    "a\n",
    "t\n",
    ",\n",
    "s\n",
    "t\n",
    "+\n",
    "1\n",
    ")\n",
    "]\n",
    "Vπ(s)=E[∑ \n",
    "t=0\n",
    "∞\n",
    "​\n",
    " γ \n",
    "t\n",
    " R(s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " ,s \n",
    "t+1\n",
    "​\n",
    " )]\n",
    "\n",
    "It represents the expected cumulative reward from state \n",
    "s\n",
    "s under policy \n",
    "π\n",
    "π.\n",
    "\n",
    "2. Action Value Function (\n",
    "Q\n",
    "π\n",
    "(\n",
    "s\n",
    ",\n",
    "a\n",
    ")\n",
    "Qπ(s,a)):\n",
    "\n",
    "Q\n",
    "π\n",
    "(\n",
    "s\n",
    ",\n",
    "a\n",
    ")\n",
    "=\n",
    "E\n",
    "[\n",
    "R\n",
    "(\n",
    "s\n",
    ",\n",
    "a\n",
    ",\n",
    "s\n",
    "′\n",
    ")\n",
    "+\n",
    "γ\n",
    "V\n",
    "π\n",
    "(\n",
    "s\n",
    "′\n",
    ")\n",
    "]\n",
    "Qπ(s,a)=E[R(s,a,s \n",
    "′\n",
    " )+γVπ(s \n",
    "′\n",
    " )]\n",
    "\n",
    "It evaluates the expected reward of taking action a in state s and following \n",
    "π\n",
    "πthereafter.\n",
    "\n",
    "The Optimal Value Function is defined as:\n",
    "\n",
    "Optimal State Value Function (V(s)):\n",
    "V\n",
    "∗\n",
    "(\n",
    "s\n",
    ")\n",
    "=\n",
    "max\n",
    "⁡\n",
    "a\n",
    "Q\n",
    "∗\n",
    "(\n",
    "s\n",
    ",\n",
    "a\n",
    ")\n",
    "V \n",
    "∗\n",
    " (s)=max \n",
    "a\n",
    "​\n",
    " Q \n",
    "∗\n",
    " (s,a)\n",
    "'''\n",
    "summary = pipe(texts)\n",
    "print(summary[0]['summary_text'])\n",
    "\n",
    "def summary(text):\n",
    "    summarized_text = pipe(text)\n",
    "    return summarized_text[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()\n",
    "\n",
    "demo = gr.Interface(fn=summary, inputs=\"text\", outputs=\"text\", title=\"Text Summarizer\", description=\"Enter text to summarize using a pre-trained model.\")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
