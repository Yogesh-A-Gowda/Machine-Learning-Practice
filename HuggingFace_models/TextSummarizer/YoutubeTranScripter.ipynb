{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr1JdRxuYmcP",
        "outputId": "d6bada81-18f1-4df6-b043-3d903c60db16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: youtube_transcript_api in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from youtube_transcript_api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from youtube_transcript_api) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->youtube_transcript_api) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->youtube_transcript_api) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->youtube_transcript_api) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->youtube_transcript_api) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install youtube_transcript_api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "your_token = os.getenv(\"HF\")\n",
        "# Log in using the token\n",
        "login(token=your_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofhBpzhzbssv",
        "outputId": "1a79474f-5262-4a65-e962-28e5b2bf2792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original transcript token length = 1110\n",
            "[iter 1] combined token length = 110; SAFE_CHUNK_TOKENS = 960\n",
            "\n",
            "=== FINAL SUMMARY ===\n",
            "\n",
            "Khloe's breakup with Connor is one of the most emotionally devastating moments of the series. A father-son storyline where Jim discovers creativity through Connor could deepen both characters. By weaving Connor into everyone's arcs, Georgie and Mandy's first marriage can transform him.\n",
            "Original transcript token length = 10003\n",
            "[iter 1] combined token length = 623; SAFE_CHUNK_TOKENS = 960\n",
            "\n",
            "=== FINAL SUMMARY ===\n",
            "\n",
            "This is a sentences. These are the sentences that make up a sentence, or a series of sentences, or even just a single sentence. They all start with the same word, and end with a different word, or sometimes even a different sentence.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "import re\n",
        "import math\n",
        "\n",
        "# -------------------------\n",
        "# Model & tokenizer\n",
        "# -------------------------\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True\n",
        ").to(\"cuda\")\n",
        "\n",
        "# true maximum token window the model supports\n",
        "MODEL_MAX_TOKENS = getattr(model.config, \"max_position_embeddings\", None) or getattr(tokenizer, \"model_max_length\", None)\n",
        "if MODEL_MAX_TOKENS is None:\n",
        "    # fallback safe cap\n",
        "    MODEL_MAX_TOKENS = 1024\n",
        "\n",
        "# keep a safety margin so generation's own max_length fits\n",
        "SAFETY_MARGIN = 64\n",
        "SAFE_CHUNK_TOKENS = max(64, MODEL_MAX_TOKENS - SAFETY_MARGIN)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Token utilities\n",
        "# -------------------------\n",
        "def encode_ids(text, add_special_tokens=True):\n",
        "    enc = tokenizer(text, return_tensors=\"pt\", truncation=False, add_special_tokens=add_special_tokens)\n",
        "    return enc.input_ids[0]\n",
        "\n",
        "def tokens_to_text(token_ids):\n",
        "    return tokenizer.decode(token_ids, skip_special_tokens=True)\n",
        "\n",
        "def split_token_tensor(token_ids, max_tokens=SAFE_CHUNK_TOKENS):\n",
        "    \"\"\"Split a 1D token tensor into list of 1D token tensors each <= max_tokens.\"\"\"\n",
        "    L = token_ids.size(0)\n",
        "    return [token_ids[i:i+max_tokens] for i in range(0, L, max_tokens)]\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Safe summarization primitives\n",
        "# -------------------------\n",
        "def generate_from_token_chunk(token_chunk, max_length=150, min_length=20, num_beams=4):\n",
        "    \"\"\"\n",
        "    token_chunk: 1D torch tensor of token ids (NOT batched). THIS function ensures device placement and calls generate.\n",
        "    \"\"\"\n",
        "    if token_chunk.size(0) > SAFE_CHUNK_TOKENS:\n",
        "        raise ValueError(f\"generate_from_token_chunk called with {token_chunk.size(0)} tokens > SAFE_CHUNK_TOKENS {SAFE_CHUNK_TOKENS}\")\n",
        "\n",
        "    input_ids = token_chunk.unsqueeze(0).to(model.device)\n",
        "    out = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        num_beams=num_beams,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "def summarize_text_safe(text, max_length=150, min_length=20):\n",
        "    \"\"\"\n",
        "    Summarize an arbitrary-length text by:\n",
        "     - tokenizing\n",
        "     - splitting into safe token chunks\n",
        "     - generating per-chunk summaries\n",
        "     - returning list of chunk summaries\n",
        "    This DOES NOT attempt to produce the final merged summary.\n",
        "    \"\"\"\n",
        "    ids = encode_ids(text)\n",
        "    if ids.size(0) == 0:\n",
        "        return []\n",
        "\n",
        "    token_chunks = split_token_tensor(ids, SAFE_CHUNK_TOKENS)\n",
        "    summaries = []\n",
        "    for i, tk in enumerate(token_chunks):\n",
        "        # defensive check\n",
        "        if tk.size(0) > SAFE_CHUNK_TOKENS:\n",
        "            tk = tk[:SAFE_CHUNK_TOKENS]\n",
        "        s = generate_from_token_chunk(tk, max_length=max_length, min_length=min_length)\n",
        "        summaries.append(s)\n",
        "    return summaries\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Iterative reducer (multi-pass)\n",
        "# -------------------------\n",
        "def iterative_compress(text, per_chunk_max_len=150, per_chunk_min_len=20, final_max_len=200):\n",
        "    \"\"\"\n",
        "    Repeatedly summarize until the tokenized combined summary fits into SAFE_CHUNK_TOKENS.\n",
        "    This avoids ever calling model.generate with too many tokens.\n",
        "    \"\"\"\n",
        "    # 1) first pass: summarize original into chunk summaries\n",
        "    summaries = summarize_text_safe(text, max_length=per_chunk_max_len, min_length=per_chunk_min_len)\n",
        "    if not summaries:\n",
        "        return \"\"\n",
        "\n",
        "    # if there's one chunk only, return it (maybe further compress if still long)\n",
        "    combined = \" \".join(summaries)\n",
        "\n",
        "    # iterative reduction loop: while combined is too long in tokens, compress its parts again\n",
        "    iter_count = 0\n",
        "    while True:\n",
        "        iter_count += 1\n",
        "        token_len = encode_ids(combined).size(0)\n",
        "        # debug info\n",
        "        print(f\"[iter {iter_count}] combined token length = {token_len}; SAFE_CHUNK_TOKENS = {SAFE_CHUNK_TOKENS}\")\n",
        "\n",
        "        if token_len <= SAFE_CHUNK_TOKENS:\n",
        "            # safe to produce final summary (one last generation)\n",
        "            final_ids = encode_ids(combined)\n",
        "            # ensure safe truncation just in case\n",
        "            if final_ids.size(0) > SAFE_CHUNK_TOKENS:\n",
        "                final_ids = final_ids[:SAFE_CHUNK_TOKENS]\n",
        "            final_summary = generate_from_token_chunk(final_ids, max_length=final_max_len, min_length=per_chunk_min_len)\n",
        "            return final_summary\n",
        "\n",
        "        # otherwise combined is too long — compress it by summarizing its token-chunks\n",
        "        # split combined into smaller pieces and summarize each piece\n",
        "        compressed_parts = summarize_text_safe(combined, max_length=per_chunk_max_len, min_length=per_chunk_min_len)\n",
        "        combined = \" \".join(compressed_parts)\n",
        "\n",
        "        # safety: prevent infinite loops — if no size reduction achieved, truncate\n",
        "        if iter_count >= 6:\n",
        "            print(\"[warning] reached maximum iterations; truncating combined to SAFE_CHUNK_TOKENS and summarizing final chunk.\")\n",
        "            final_ids = encode_ids(combined)[:SAFE_CHUNK_TOKENS]\n",
        "            return generate_from_token_chunk(final_ids, max_length=final_max_len, min_length=per_chunk_min_len)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# YouTube utilities\n",
        "# -------------------------\n",
        "def extract_youtube_id(url):\n",
        "    if not url:\n",
        "        return None\n",
        "    parsed = urlparse(url)\n",
        "    hostname = parsed.hostname or \"\"\n",
        "    if hostname.endswith(\"youtube.com\") or hostname == \"m.youtube.com\":\n",
        "        q = parse_qs(parsed.query)\n",
        "        if \"v\" in q:\n",
        "            return q[\"v\"][0]\n",
        "        m = re.search(r\"/(?:embed|v|shorts)/([A-Za-z0-9_-]{11})\", parsed.path)\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "    if hostname == \"youtu.be\":\n",
        "        vid = parsed.path.strip(\"/\")\n",
        "        if len(vid) == 11:\n",
        "            return vid\n",
        "    return None\n",
        "\n",
        "# def youtubeTranscript(url):\n",
        "#     video_id = extract_youtube_id(url)\n",
        "#     if not video_id:\n",
        "#         return \"Invalid YouTube URL\"\n",
        "#     try:\n",
        "#         transcript = YouTubeTranscriptApi().fetch(video_id)\n",
        "#         finalSTR=''\n",
        "#         for i in range(len(transcript)):\n",
        "#             tempSTR = str(transcript[i])\n",
        "#             finalSTR += tempSTR[24:] + ' '\n",
        "#         return finalSTR\n",
        "#     except Exception as e:\n",
        "#         return f\"Error fetching transcript: {str(e)}\"\n",
        "# def youtubeTranscript(url):\n",
        "#     video_id = extract_youtube_id(url)\n",
        "#     if not video_id:\n",
        "#         return \"Invalid YouTube URL\"\n",
        "#     try:\n",
        "#         transcript = YouTubeTranscriptApi().fetch(video_id)\n",
        "#         finalSTR = \"\"\n",
        "#         for item in transcript:\n",
        "#             finalSTR += item[\"text\"] + \" \"\n",
        "#         return finalSTR\n",
        "#     except Exception as e:\n",
        "#         return f\"Error fetching transcript: {str(e)}\"\n",
        "\n",
        "# def youtubeTranscript(url):\n",
        "#     video_id = extract_youtube_id(url)\n",
        "#     if not video_id:\n",
        "#         return \"Invalid YouTube URL\"\n",
        "#     try:\n",
        "#         transcript = YouTubeTranscriptApi().fetch(video_id)\n",
        "#         final = []\n",
        "#         for item in transcript:\n",
        "#             try:\n",
        "#                 # Normal format\n",
        "#                 final.append(item[\"text\"])\n",
        "#             except:\n",
        "#                 # Snippet fallback\n",
        "#                 final.append(item.text)\n",
        "\n",
        "#         return \" \".join(final)\n",
        "\n",
        "#     except Exception as e:\n",
        "#         return f\"Error fetching transcript: {str(e)}\"\n",
        "\n",
        "# def youtubeTranscript(url):\n",
        "#     video_id = extract_youtube_id(url)\n",
        "#     if not video_id:\n",
        "#         return \"Invalid YouTube URL\"\n",
        "#     try:\n",
        "#         # try english first\n",
        "#         transcript = YouTubeTranscriptApi().list_transcripts(video_id)\n",
        "\n",
        "#         # priority:\n",
        "#         preferred = None\n",
        "\n",
        "#         # 1. manually written English (best)\n",
        "#         if transcript.find_transcript(['en'],).is_translatable:\n",
        "#             preferred = transcript.find_transcript(['en'])\n",
        "\n",
        "#         # 2. auto-generated English\n",
        "#         if preferred is None:\n",
        "#             try:\n",
        "#                 preferred = transcript.find_transcript(['en'])\n",
        "#             except:\n",
        "#                 pass\n",
        "\n",
        "#         # 3. fallback Kannada\n",
        "#         if preferred is None:\n",
        "#             preferred = transcript.find_transcript(['kn'])\n",
        "\n",
        "#         fetched = preferred.fetch()\n",
        "\n",
        "#         final = []\n",
        "#         for item in fetched:\n",
        "#             try:\n",
        "#                 final.append(item[\"text\"])\n",
        "#             except:\n",
        "#                 final.append(item.text)\n",
        "\n",
        "#         return \" \".join(final)\n",
        "\n",
        "#     except Exception as e:\n",
        "#         return f\"Error fetching transcript: {str(e)}\"\n",
        "\n",
        "\n",
        "\n",
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
        "\n",
        "def youtubeTranscript(url):\n",
        "    video_id = extract_youtube_id(url)\n",
        "    if not video_id:\n",
        "        return \"Invalid YouTube URL\"\n",
        "\n",
        "    try:\n",
        "        ytt = YouTubeTranscriptApi()  # create instance\n",
        "        transcript_list = ytt.list(video_id)\n",
        "\n",
        "        # Try to find English transcript, fallback if needed\n",
        "        try:\n",
        "            transcript = transcript_list.find_transcript(['en'])\n",
        "        except Exception:\n",
        "            transcript = None\n",
        "\n",
        "        if transcript is None:\n",
        "            # fallback: use first available transcript\n",
        "            transcript = next(iter(transcript_list), None)\n",
        "\n",
        "        if transcript is None:\n",
        "            return \"No transcript found\"\n",
        "\n",
        "        fetched = transcript.fetch()\n",
        "\n",
        "        texts = []\n",
        "        for snippet in fetched:\n",
        "            # snippet is a FetchedTranscriptSnippet\n",
        "            texts.append(snippet.text)\n",
        "\n",
        "        return \" \".join(texts)\n",
        "\n",
        "    except TranscriptsDisabled:\n",
        "        return \"Transcripts are disabled for this video\"\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching transcript: {str(e)}\"\n",
        "\n",
        "# -------------------------\n",
        "# High-level final function\n",
        "# -------------------------\n",
        "def final(url_or_text, is_url=True):\n",
        "    if is_url:\n",
        "        txt = youtubeTranscript(url_or_text)\n",
        "        if txt.startswith(\"Invalid\") or txt.startswith(\"Error\"):\n",
        "            print(txt)\n",
        "            return txt\n",
        "    else:\n",
        "        txt = url_or_text\n",
        "\n",
        "    print(f\"Original transcript token length = {encode_ids(txt).size(0)}\")\n",
        "    result = iterative_compress(txt, per_chunk_max_len=150, per_chunk_min_len=40, final_max_len=200)\n",
        "    print(\"\\n=== FINAL SUMMARY ===\\n\")\n",
        "    print(result)\n",
        "    return result\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Example run\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Test with either a YouTube URL or huge text.\n",
        "    final(\"https://www.youtube.com/watch?v=AMX1kwIASZ4\", is_url=True)\n",
        "    # Or test with a very long string to simulate the transcript:\n",
        "    # long_text = \"This is a sentence. \" * 2000\n",
        "    # final(long_text, is_url=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "6BI0PTOVfL0H",
        "outputId": "fe8c989a-1985-4a97-d021-022a663524e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8babde29255366f4b2.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://8babde29255366f4b2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "gr.close_all()\n",
        "\n",
        "demo = gr.Interface(fn=final,\n",
        "                    inputs=[gr.Textbox(label=\"Input YouTube Url to summarize\")],\n",
        "                    outputs=[gr.Textbox(label=\"Summarized text\")],\n",
        "                    title=\"YouTube Script Summarizer\",\n",
        "                    description=\"SUMMARIZE THE YOUTUBE VIDEO SCRIPT.\")\n",
        "demo.launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
